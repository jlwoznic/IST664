{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import sentence_polarity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sentence_polarity.sents()\n",
    "for sent in sentences[:4]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n"
     ]
    }
   ],
   "source": [
    "pos_sents = sentence_polarity.sents(categories=\"pos\")\n",
    "neg_sents = sentence_polarity.sents(categories=\"neg\")\n",
    "print(len(pos_sents))\n",
    "print(len(neg_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')\n",
      "(['provides', 'a', 'porthole', 'into', 'that', 'noble', ',', 'trembling', 'incoherence', 'that', 'defines', 'us', 'all', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "documents = [(sent, cat) for cat in sentence_polarity.categories() for sent in sentence_polarity.sents(categories=cat)]\n",
    "\n",
    "print(documents[0])\n",
    "print(documents[-1])\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'but', 'with', 'film', 'this', 'for', 'its', 'an', 'movie', \"it's\", 'be', 'on', 'you', 'not', 'by', 'about', 'more', 'one', 'like', 'has', 'are', 'at', 'from', 'than', '\"', 'all', '--', 'his', 'have', 'so', 'if', 'or', 'story', 'i', 'too', 'just', 'who', 'into', 'what']\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent, cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features: \n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.756"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     21.7 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     15.6 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.6 : 1.0\n",
      "                V_flawed = True              pos : neg    =     15.0 : 1.0\n",
      "              V_supposed = True              neg : pos    =     15.0 : 1.0\n",
      "               V_routine = True              neg : pos    =     14.3 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.8 : 1.0\n",
      "                  V_flat = True              neg : pos    =     13.4 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     13.0 : 1.0\n",
      "             V_inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                  V_warm = True              pos : neg    =     12.2 : 1.0\n",
      "                  V_dull = True              neg : pos    =     11.9 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     11.8 : 1.0\n",
      "           V_mesmerizing = True              pos : neg    =     11.7 : 1.0\n",
      "                 V_stale = True              neg : pos    =     11.7 : 1.0\n",
      "           V_superficial = True              neg : pos    =     11.7 : 1.0\n",
      "              V_delicate = True              pos : neg    =     11.0 : 1.0\n",
      "                V_beauty = True              pos : neg    =     10.6 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.3 : 1.0\n",
      "                V_unless = True              neg : pos    =     10.3 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.2 : 1.0\n",
      "              V_touching = True              pos : neg    =      9.9 : 1.0\n",
      "              V_captures = True              pos : neg    =      9.8 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =      9.7 : 1.0\n",
      "             V_realistic = True              pos : neg    =      9.7 : 1.0\n",
      "             V_offensive = True              neg : pos    =      9.7 : 1.0\n",
      "              V_tiresome = True              neg : pos    =      9.7 : 1.0\n",
      "           V_pretentious = True              neg : pos    =      9.6 : 1.0\n",
      "                V_stupid = True              neg : pos    =      9.4 : 1.0\n",
      "              V_powerful = True              pos : neg    =      9.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Subjectivity reads the subjectivity lexicon file from Wiebe et al\n",
    "#    at http://www.cs.pitt.edu/mpqa/ (part of the Multiple Perspective QA project)\n",
    "#\n",
    "# This file has the format that each line is formatted as in this example for the word \"abandoned\"\n",
    "#     type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
    "# In our data, the pos tag is ignored, so this program just takes the last one read\n",
    "#     (typically the noun over the adjective)\n",
    "#\n",
    "# The data structure that is created is a dictionary where\n",
    "#    each word is mapped to a list of 4 things:  \n",
    "#        strength, which will be either 'strongsubj' or 'weaksubj'\n",
    "#        posTag, either 'adj', 'verb', 'noun', 'adverb', 'anypos'\n",
    "#        isStemmed, either true or false\n",
    "#        polarity, either 'positive', 'negative', or 'neutral'\n",
    "\n",
    "import nltk\n",
    "\n",
    "# pass the absolute path of the lexicon file to this program\n",
    "# example call:\n",
    "# nancymacpath = \n",
    "#    \"/Users/njmccrac/AAAdocs/research/subjectivitylexicon/hltemnlp05clues/subjclueslen1-HLTEMNLP05.tff\"\n",
    "# SL = readSubjectivity(nancymacpath)\n",
    "\n",
    "# this function returns a dictionary where you can look up words and get back \n",
    "#     the four items of subjectivity information described above\n",
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Subjectivity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6468daccc07c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mSLpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"subjclueslen1-HLTEMNLP05.tff\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mSubjectivity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mSL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSubjectivity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadSubjectivity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSLpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Subjectivity'"
     ]
    }
   ],
   "source": [
    "SLpath = \"subjclueslen1-HLTEMNLP05.tff\"\n",
    "import Subjectivity\n",
    "SL = Subjectivity.readSubjectivity(SLpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
