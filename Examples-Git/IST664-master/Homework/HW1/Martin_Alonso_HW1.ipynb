{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST664 Natural Language Processing \n",
    "## Homework 1 \n",
    "### Martin Alonso \n",
    "### 2019-04-27\n",
    "\n",
    "For this homework, we'll be working with two texts authored by James Joyce. The first _The Dead_ is a novella, and the last story within the _Dubliners_ collection of stories. The second, is Joyce's final, most ambitious and questionable work, _Finnegans Wake_, which works with myriad forms of words in the English language, transforming sentences in such a way that each sentence can have, at a bare minimum, three separate interpretations.  \n",
    "We will compare both of these works to understand how Joyce's use of language has changed over the course of his writing career: both in the words he most frequently uses and bigram interaction among these. Word frequency scores will also be applied to both works.  \n",
    "Finally, we want to answer how Joyce's choice of word pairs changes over time, allowing him to build more complex mental imagery that allows for far more interpretative possibilites than at the beginning of his, more straightforward, writing career. For this, we will see how Joyce's initial writing focuses more on characters while his last work focuses more on the use of actions, along with adjectives, adverbs, and nouns, rather than multiple characters and actions performed by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining the Corpora\n",
    "Both corpora were obtained from the University of Adelaide's library collection. The books were downloaded in ebook format and converted to text files outside the Python script.  \n",
    "No further text manipulation was done to either text, keeping the title, author, and entire corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "We will begin loading the _nltk_ and *re* packages that will allow us to process both texts and do some initial cleaning, such as lowercasing words and stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open texts. Latin-1 encoding is used because of Joyce's penchance for borrowing words (and characters) from other languages. \n",
    "text1 = open(\"James Joyce The Dead.txt\", \"r\", encoding=\"latin-1\")\n",
    "text2 = open(\"James Joyce Finnegans Wake.txt\", \"r\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the texts\n",
    "the_dead = text1.read()\n",
    "finnegans_wake = text2.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first lowercase every word within each corpus. Then, we will remove stopwords from both corpora; however, we will not add any initial stopwords to nltk's stopwords list. If any additional words show up within the corpora that are deemed unnecessary, these words will then be added and will be reprocessed.  \n",
    "Finally, we will stem every token within the corpora. But this event will come with two caveats. Given how Joyce uses complex language and sometimes replaces certain letters within words to create different meanings and puns, we will limit ourselves to using the less conservative Porter stemmer. Nevertheless, this could also create several, very similar, tokens, especially within _Finnegans Wake_, which may lead to many similar tokens and bigrams.  \n",
    "But, we look forward to finding these as it could lead to more interesting insight into Joyce's idiosyncratic use of language to create mental imagery.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dead: 17983 tokens\n",
      "Finnegans Wake: 258500 tokens\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words to check length of corpora\n",
    "the_dead_tokens = nltk.word_tokenize(the_dead)\n",
    "finnegans_wake_tokens = nltk.word_tokenize(finnegans_wake)\n",
    "\n",
    "# Print token length\n",
    "print('The Dead: {} tokens'.format(len(the_dead_tokens)))\n",
    "print('Finnegans Wake: {} tokens'.format(len(finnegans_wake_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lowercase function that loops through every token in a corpus and returns a list of each lowercased token. \n",
    "def lowercase(text):\n",
    "    token_list = []\n",
    "    for word in text:\n",
    "        word_lower = word.lower()\n",
    "        token_list.append(word_lower)\n",
    "    return(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase tokens in each text for easier stop word removal, stemming, and do frequency distribution\n",
    "# for each corpus. \n",
    "td_lower = lowercase(the_dead_tokens)\n",
    "fw_lower = lowercase(finnegans_wake_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "td_lower_stop = [word for word in td_lower if word not in stop_words]\n",
    "fw_lower_stop = [word for word in fw_lower if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dead: 10548 tokens\n",
      "Finnegans Wake: 160874 tokens\n"
     ]
    }
   ],
   "source": [
    "# Count the new number of tokens now that stopwords have been removed.\n",
    "print('The Dead: {} tokens'.format(len(td_lower_stop)))\n",
    "print('Finnegans Wake: {} tokens'.format(len(fw_lower_stop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very interesting. By removing stopwords from both corpora, we see that _The Dead_ loses around 7,000 tokens. But _Finnegans Wake_ loses nearly 100,000 tokens. Proportionally, _The Dead_ loses 41.3 percent of its tokens, while _Finnegans Wake_ loses 37.8 percent of its text.  \n",
    "This may mean that, not only Joyce avoids using more traditional stopwords in his later novel but, given Joyce's character manipulation within tokens in _Finnegans Wake_, he may have created new forms of stopwords that elude nltk's stopwords stemmer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words \n",
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_stemmed = []\n",
    "for word in td_lower_stop:\n",
    "    stemmed = porter.stem(word)\n",
    "    td_stemmed.append(stemmed)\n",
    "\n",
    "fw_stemmed = []\n",
    "for word in fw_lower_stop:\n",
    "    stemmed = porter.stem(word)\n",
    "    fw_stemmed.append(stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters and print out the first 50 words for each corpus.\n",
    "# The Dead\n",
    "td_clean = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in td_stemmed]\n",
    "td_clean = list(filter(None, td_clean))\n",
    "\n",
    "#Finnegans Wake\n",
    "fw_clean = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in fw_stemmed]\n",
    "fw_clean = list(filter(None, fw_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Analysis\n",
    "We'll now move on to checking the 50 most common unigrams and bigrams for the corpora. We'll then review mutual information scores for each bigram, provided that each token within the bigram has a frequency distribution greater than four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 most common words in The Dead are:\n",
      "\n",
      "[('said', 187), ('gabriel', 158), ('aunt', 109), ('mr', 79), ('kate', 75), ('miss', 67), ('brown', 59), ('julia', 56), ('mari', 54), ('jane', 53), ('would', 50), ('ask', 48), ('one', 44), ('like', 40), ('go', 39), ('i', 39), ('well', 38), ('come', 35), ('malin', 34), ('face', 34), ('freddi', 33), ('hand', 33), ('voic', 33), ('eye', 32), ('turn', 31), ('o', 31), ('good', 29), ('look', 29), ('back', 27), ('see', 27), ('could', 26), ('it', 26), ('ivor', 26), ('room', 25), ('know', 25), ('darci', 25), ('never', 24), ('long', 24), ('moment', 24), ('gretta', 24), ('old', 23), ('think', 23), ('came', 22), ('year', 22), ('still', 22), ('mrs', 22), ('smile', 22), ('he', 22), ('head', 21), ('answer', 21)]\n"
     ]
    }
   ],
   "source": [
    "# Use NLTK's FreqDist function to calculate token counts on the cleaned corpora\n",
    "td_freq = nltk.FreqDist(td_clean)\n",
    "fw_freq = nltk.FreqDist(fw_clean)\n",
    "\n",
    "# Print the 50 most common tokens in The Dead...\n",
    "print(\"The 50 most common words in The Dead are:\\n\")\n",
    "print(td_freq.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 most common words in Finnegans Wake are:\n",
      "\n",
      "[('one', 686), ('like', 589), ('old', 469), ('would', 355), ('time', 342), ('say', 311), ('us', 306), ('two', 287), ('well', 280), ('come', 271), ('man', 269), ('may', 269), ('till', 268), ('see', 267), ('let', 254), ('make', 246), ('first', 237), ('go', 226), ('way', 224), ('know', 218), ('good', 218), ('ever', 207), ('tell', 195), ('look', 192), ('could', 192), ('never', 185), ('love', 181), ('ye', 180), ('take', 177), ('upon', 172), ('three', 172), ('day', 169), ('everi', 163), ('back', 162), ('still', 162), ('littl', 157), ('hear', 156), ('call', 153), ('mr', 152), ('hand', 148), ('it', 148), ('night', 147), ('made', 147), ('ill', 146), ('round', 143), ('long', 141), ('yet', 140), ('eye', 139), ('shall', 138), ('name', 138)]\n"
     ]
    }
   ],
   "source": [
    "# ...and the 50 most common in Finnegans Wake. \n",
    "print(\"The 50 most common words in Finnegans Wake are:\\n\")\n",
    "print(fw_freq.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note how Joyce in _The Dead_ is more character driven than in _Finnegans Wake_. The most common words in the first corpus refer to characters and titles and actions regarding those characters, especially when it comes to what each character says.  \n",
    "However, _Finnegans Wake_ is more focused on actions which is interesting given that the novel is very convoluted and there is no clear storyline but a stream of ideas that are open to interpretation.  \n",
    "We'll now repeat the same exercise for bigrams, finding the 50 most common bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bigram finder and score bigram frequency\n",
    "# We'll first work with The Dead \n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "td_finder = BigramCollocationFinder.from_words(td_clean)\n",
    "td_scored = td_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('aunt', 'kate'), 0.008059592135791917)\n",
      "(('mari', 'jane'), 0.006472096715105629)\n",
      "(('mr', 'brown'), 0.005373061423861277)\n",
      "(('aunt', 'julia'), 0.0041519111002564415)\n",
      "(('said', 'aunt'), 0.0035413359384540237)\n",
      "(('said', 'gabriel'), 0.0032971058737330567)\n",
      "(('freddi', 'malin'), 0.002930760776651606)\n",
      "(('miss', 'ivor'), 0.002930760776651606)\n",
      "(('said', 'mr'), 0.001953840517767737)\n",
      "(('said', 'mari'), 0.0018317254854072536)\n",
      "(('bartel', 'darci'), 0.00170961045304677)\n",
      "(('mr', 'bartel'), 0.0015874954206862866)\n",
      "(('mrs', 'conroy'), 0.001465380388325803)\n",
      "(('mr', 'darci'), 0.0013432653559653192)\n",
      "(('ask', 'gabriel'), 0.0012211503236048357)\n",
      "(('miss', 'morkan'), 0.0012211503236048357)\n",
      "(('gabriel', 'said'), 0.0010990352912443521)\n",
      "(('miss', 'dali'), 0.0009769202588838686)\n",
      "(('mrs', 'malin'), 0.0009769202588838686)\n",
      "(('ask', 'mr'), 0.000854805226523385)\n",
      "(('kate', 'said'), 0.000854805226523385)\n",
      "(('ladi', 'gentlemen'), 0.000854805226523385)\n",
      "(('old', 'gentleman'), 0.000854805226523385)\n",
      "(('said', 'miss'), 0.000854805226523385)\n",
      "(('gay', 'fellow'), 0.0007326901941629015)\n",
      "(('jolli', 'gay'), 0.0007326901941629015)\n",
      "(('kate', 'aunt'), 0.0007326901941629015)\n",
      "(('miss', 'ocallaghan'), 0.0007326901941629015)\n",
      "(('young', 'men'), 0.0007326901941629015)\n",
      "(('ask', 'miss'), 0.0006105751618024178)\n",
      "(('could', 'hear'), 0.0006105751618024178)\n",
      "(('gone', 'away'), 0.0006105751618024178)\n",
      "(('michael', 'furey'), 0.0006105751618024178)\n",
      "(('miss', 'furlong'), 0.0006105751618024178)\n",
      "(('o', 'mr'), 0.0006105751618024178)\n",
      "(('said', 'i'), 0.0006105751618024178)\n",
      "(('said', 'mrs'), 0.0006105751618024178)\n",
      "(('young', 'man'), 0.0006105751618024178)\n",
      "(('fellow', 'jolli'), 0.0004884601294419343)\n",
      "(('freddi', 'malins'), 0.0004884601294419343)\n",
      "(('i', 'think'), 0.0004884601294419343)\n",
      "(('im', 'sure'), 0.0004884601294419343)\n",
      "(('lass', 'aughrim'), 0.0004884601294419343)\n",
      "(('miss', 'julia'), 0.0004884601294419343)\n",
      "(('miss', 'kate'), 0.0004884601294419343)\n",
      "(('mr', 'conroy'), 0.0004884601294419343)\n",
      "(('never', 'heard'), 0.0004884601294419343)\n",
      "(('nod', 'head'), 0.0004884601294419343)\n",
      "(('of', 'cours'), 0.0004884601294419343)\n",
      "(('poor', 'fellow'), 0.0004884601294419343)\n"
     ]
    }
   ],
   "source": [
    "# Print the first 50 bigram scores for The Dead\n",
    "for bscore in td_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the bigrams for _The Dead_, the idea posited by looking at the token frequency distribution confirms that most of the novella is character driven through conversation. The bigrams either refer to a character or what a character has said, done, or asked.  \n",
    "We'll now review the bigrams for _Finnegans Wake_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('let', 'us'), 0.00030433638218069355)\n",
      "(('poor', 'old'), 0.0001316049220240837)\n",
      "(('come', 'back'), 0.00012337961439757848)\n",
      "(('anna', 'livia'), 0.000106928999144568)\n",
      "(('look', 'like'), 0.000106928999144568)\n",
      "(('one', 'one'), 0.000106928999144568)\n",
      "(('ah', 'ho'), 9.870369151806278e-05)\n",
      "(('tell', 'us'), 9.870369151806278e-05)\n",
      "(('one', 'time'), 9.047838389155755e-05)\n",
      "(('shaun', 'repli'), 9.047838389155755e-05)\n",
      "(('tell', 'tell'), 9.047838389155755e-05)\n",
      "(('would', 'like'), 9.047838389155755e-05)\n",
      "(('ay', 'ay'), 8.225307626505231e-05)\n",
      "(('everi', 'time'), 7.402776863854709e-05)\n",
      "(('grand', 'old'), 7.402776863854709e-05)\n",
      "(('one', 'two'), 7.402776863854709e-05)\n",
      "(('see', 'see'), 7.402776863854709e-05)\n",
      "(('thousand', 'one'), 7.402776863854709e-05)\n",
      "(('could', 'tell'), 6.580246101204185e-05)\n",
      "(('hide', 'seek'), 6.580246101204185e-05)\n",
      "(('old', 'man'), 6.580246101204185e-05)\n",
      "(('one', 'day'), 6.580246101204185e-05)\n",
      "(('say', 'noth'), 6.580246101204185e-05)\n",
      "(('wait', 'till'), 6.580246101204185e-05)\n",
      "(('well', 'know'), 6.580246101204185e-05)\n",
      "(('would', 'go'), 6.580246101204185e-05)\n",
      "(('come', 'ye'), 5.757715338553662e-05)\n",
      "(('dont', 'forget'), 5.757715338553662e-05)\n",
      "(('dont', 'know'), 5.757715338553662e-05)\n",
      "(('ever', 'sinc'), 5.757715338553662e-05)\n",
      "(('first', 'last'), 5.757715338553662e-05)\n",
      "(('give', 'us'), 5.757715338553662e-05)\n",
      "(('good', 'old'), 5.757715338553662e-05)\n",
      "(('ho', 'ho'), 5.757715338553662e-05)\n",
      "(('like', 'old'), 5.757715338553662e-05)\n",
      "(('night', 'night'), 5.757715338553662e-05)\n",
      "(('nin', 'nin'), 5.757715338553662e-05)\n",
      "(('part', 'say'), 5.757715338553662e-05)\n",
      "(('right', 'enough'), 5.757715338553662e-05)\n",
      "(('two', 'three'), 5.757715338553662e-05)\n",
      "(('four', 'us'), 4.935184575903139e-05)\n",
      "(('hand', 'hand'), 4.935184575903139e-05)\n",
      "(('hold', 'hard'), 4.935184575903139e-05)\n",
      "(('ill', 'take'), 4.935184575903139e-05)\n",
      "(('im', 'sorri'), 4.935184575903139e-05)\n",
      "(('may', 'never'), 4.935184575903139e-05)\n",
      "(('n', 'k'), 4.935184575903139e-05)\n",
      "(('number', 'one'), 4.935184575903139e-05)\n",
      "(('order', 'order'), 4.935184575903139e-05)\n",
      "(('pleas', 'stop'), 4.935184575903139e-05)\n"
     ]
    }
   ],
   "source": [
    "# Bigram scorer for Finnegans Wake\n",
    "fw_finder = BigramCollocationFinder.from_words(fw_clean)\n",
    "fw_scored = fw_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "# Print the first 50 bigram scores for Finnegans Wake\n",
    "for bscore in fw_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that's noticeable about the 50 most common bigrams is that there are many that are word repetitions. 'Ay ay', 'ho ho', and 'hand hand' are some examples. The second thing to notice is that there are almost no characters as compared to _The Dead_. Ther are three (Anna Livia, Shaun, and old man) but it's a much reduced number. What's also interesting is that there are instances of counting within the corpus that repeat themselves ('one two', 'two three').  \n",
    "But, though most of the text is action based, and for the most seems like rambling non-sense, there is a bigram ('four', 'us') that shows how Joyce borrows the word 'four', replacing 'for' to create a new meaning. It would take looking at this entire sentence to (slightly) understand why this change was made. Nevertheless, we can appreciate how he mixes up word usage to convey multiple ideas.  \n",
    "  \n",
    "Finally, let's look at bigrams using Pointwise Mutual Information to see if there are any changes whatsoever to the most frequent token combinations and what they can tell us about these two corpora. For this last part, we'll use the pmi funtion rather than Raw Frequency because the former gives us single event measurements of bigram occurence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('michael', 'furey'), 10.677543477645322)\n",
      "(('jolli', 'gay'), 10.192116650475079)\n",
      "(('gay', 'fellow'), 8.999471572532684)\n",
      "(('old', 'gentleman'), 8.283264538533277)\n",
      "(('bartel', 'darci'), 8.256079709207047)\n",
      "(('mrs', 'conroy'), 8.037539613366205)\n",
      "(('young', 'men'), 7.803074359729179)\n",
      "(('ladi', 'gentlemen'), 7.784458681561835)\n",
      "(('gone', 'away'), 7.6071541497539235)\n",
      "(('freddi', 'malin'), 7.452577112645049)\n",
      "(('mari', 'jane'), 7.244584070369218)\n",
      "(('miss', 'dali'), 6.933382382074912)\n",
      "(('miss', 'ocallaghan'), 6.933382382074912)\n",
      "(('miss', 'furlong'), 6.93338238207491)\n",
      "(('miss', 'ivor'), 6.817905164654977)\n",
      "(('young', 'man'), 6.677543477645321)\n",
      "(('could', 'hear'), 6.620959949278953)\n",
      "(('mr', 'bartel'), 6.489239946888153)\n",
      "(('mrs', 'malin'), 6.452577112645047)\n",
      "(('miss', 'morkan'), 6.447955554904668)\n",
      "(('mr', 'brown'), 6.2724793936310395)\n",
      "(('aunt', 'kate'), 6.046862676618332)\n",
      "(('aunt', 'julia'), 5.511395166948493)\n",
      "(('mr', 'darci'), 5.511266253218151)\n",
      "(('o', 'mr'), 4.063422608856067)\n",
      "(('ask', 'mr'), 3.9180832456920296)\n",
      "(('ask', 'miss'), 3.670347976241116)\n",
      "(('said', 'mari'), 3.604580206090098)\n",
      "(('said', 'aunt'), 3.5423737829956927)\n",
      "(('ask', 'gabriel'), 3.432656418521786)\n",
      "(('said', 'mrs'), 3.315073588895112)\n",
      "(('said', 'mr'), 3.1487963644679446)\n",
      "(('said', 'gabriel'), 2.903683866631413)\n",
      "(('kate', 'aunt'), 2.587431057981034)\n",
      "(('said', 'i'), 2.48910298867016)\n",
      "(('said', 'miss'), 2.1938428442448803)\n",
      "(('kate', 'said'), 2.031113344206771)\n",
      "(('gabriel', 'said'), 1.318721365910255)\n"
     ]
    }
   ],
   "source": [
    "# Apply a mininmum word frequency of five to The Dead and print the top 50 bigrams. \n",
    "td_finder.apply_freq_filter(5)\n",
    "td_scored = td_finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in td_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mi_like and setting a five word frequency limit, we find that there are only 38 qualifying bigrams. Furthermore, these all refer to characters and actions associated with those characters. Joyce's first novella, thus is mostly a work on several characters and how these act and address one another, rather than being a work more subjected to a repeating idea or ideas, like _Finnegans Wake_ appears to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('whoish', 'whoish'), 13.598717183444904)\n",
      "(('nin', 'nin'), 13.054997664955627)\n",
      "(('puff', 'pive'), 12.98460833706423)\n",
      "(('jarl', 'von'), 12.343062307976707)\n",
      "(('von', 'hoother'), 12.343062307976707)\n",
      "(('hee', 'hee'), 12.0435020261178)\n",
      "(('anna', 'livia'), 11.930160553041855)\n",
      "(('n', 'k'), 11.869131119644297)\n",
      "(('marcu', 'lyon'), 11.705632387361415)\n",
      "(('rann', 'rann'), 11.399645836343074)\n",
      "(('shaun', 'repli'), 10.579441081809447)\n",
      "(('wather', 'part'), 10.448555436824021)\n",
      "(('hide', 'seek'), 9.59229091428547)\n",
      "(('hat', 'lipoleum'), 9.284168618923138)\n",
      "(('rain', 'rain'), 9.136611430509284)\n",
      "(('ay', 'ay'), 8.812547591277927)\n",
      "(('six', 'seven'), 8.496607566230162)\n",
      "(('ho', 'ho'), 8.35400317078736)\n",
      "(('ah', 'ho'), 8.296241451223715)\n",
      "(('hold', 'hard'), 8.27863043538171)\n",
      "(('tri', 'hide'), 7.975619553836976)\n",
      "(('im', 'sorri'), 7.913742006344588)\n",
      "(('stone', 'stone'), 7.8125475912779265)\n",
      "(('order', 'order'), 7.8106814050644235)\n",
      "(('la', 'bell'), 7.681558252382558)\n",
      "(('right', 'enough'), 7.520500231217458)\n",
      "(('dont', 'forget'), 7.463138759968455)\n",
      "(('half', 'hat'), 7.191059214531657)\n",
      "(('pleas', 'stop'), 7.0058025593333575)\n",
      "(('far', 'away'), 6.932366355883092)\n",
      "(('left', 'behind'), 6.833507209913574)\n",
      "(('ring', 'round'), 6.8056281773381375)\n",
      "(('stop', 'pleas'), 6.742768153499561)\n",
      "(('took', 'place'), 6.678151650939309)\n",
      "(('ah', 'ah'), 6.197837747162719)\n",
      "(('wait', 'till'), 6.070522240051508)\n",
      "(('show', 'show'), 5.868576343617121)\n",
      "(('let', 'us'), 5.854879768836881)\n",
      "(('say', 'noth'), 5.803373240484543)\n",
      "(('shall', 'pass'), 5.729863139435018)\n",
      "(('ever', 'heard'), 5.637297020698945)\n",
      "(('thousand', 'one'), 5.594965048583795)\n",
      "(('dont', 'know'), 5.422874889754729)\n",
      "(('grand', 'old'), 5.406620108075025)\n",
      "(('come', 'back'), 5.376390484042769)\n",
      "(('night', 'night'), 5.299509165057623)\n",
      "(('number', 'two'), 5.2411651788268205)\n",
      "(('ever', 'sinc'), 5.224007183706366)\n",
      "(('poor', 'old'), 5.2106998980997705)\n",
      "(('tell', 'tell'), 5.136269923810824)\n"
     ]
    }
   ],
   "source": [
    "# Repeat the same exercise for Finnegans Wake\n",
    "fw_finder.apply_freq_filter(5)\n",
    "fw_scored = fw_finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in fw_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we find that most bigrams in _Finnegans Wake_ feature word repetition, which may evoke a character's impatience (or perhaps the author's impatience) to keep the action flowing. There are many descriptive bigrams that either involve adjectives, nouns, or adverbs, but there is no clear idea or character. Except for 'anna livia', who seems to be a character, and the constant word repetition, most of the bigrams are action based. Joyce clearly moves away from character-centred writing to more action-based writing; which, judging by the contents within the novel, is evident given that there are only four characters in the book, compared to 13 in _The Dead_.   \n",
    "  \n",
    "We can see that, in the case of the frequency and PMI bigrams for both texts, there isn't much difference. Both bigrams for _The Dead_ reference multiple characters appearing through out the novella. Furthermore, these characters appear either referrencing their title, familial relationship, or with the word 'said', meaning that characters are mostly speaking or being referred to by other characters.  \n",
    "  \n",
    "Regarding _Finnegans Wake_, the bigrams are mostly similar, though, using PMI, we can see that the focus is more on word repetition than in the raw frequency distribution. The issue with _Finnegans Wake_ is that the novel does not make much sense at face value, and it needs a more in depth reading to draw any definite idea of what the novel is about. The raw frequency shows that most bigrams are action driven, joined by adjectives or adverbs; this lessens in the PMI. \n",
    "\n",
    "Overall, there aren't many differences between both novels' bigrams. However, there is a difference between both corpora, denoting how Joyce's writing style evolves from the last story in _Dubliners_, his first collection of short stories, to his final work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "As we can see, there is a definite difference to Joyce's writing style. The evolution of his writing style takes place over 25 years, starting with _The Dead_ being published as part of _Dubliners_ in 1914, and ending with _Finnegans Wake_, first published in 1939.  \n",
    "  \n",
    "It's interesting how, when contemplating Joyce's ouvreu, most of his work is very character focus. However, in _Ulysses_ there is a mix of both elements, with the novel focusing both on characters and their actions, along with surreal elements in the later parts of the novel. _Finnegans Wake_ finally submerges itself deeply into the surreal aspect of Joyce's writing, having characters taking a secondary role with the language coming to the forefront of the novel. This is evident by, sans the appearance of the bigrams 'anna livia' and 'shaun repli', no characters are mentioned within the bigrams for the novel. \n",
    "\n",
    "What is also interesting is how the bigrams are stacked together. In the case of _The Dead_, the raw frequency scores show that there is a mix between character names and these characters speaking. However, when looking at the PMI scores, it is evident that the character name bigrams are more important and thus receive higher scores. The bigrams that involve the token 'said' are still scored highly, but are grouped together among the lower scores.  \n",
    "  \n",
    "In the case of _Finnegans Wake_ the difference in how the bigrams are scored can be noticed in the number of bigrams that repeat the first and second token (i.e. 'rann, rann' or 'whoish, whoish'). These are more prominent in the PMI scores than in the raw frequency scores, but still appear in the latter. \n",
    "\n",
    "There are clear differences in Joyce's writing style between his first novella and last novel. Though these differences are evident between both corpora, the PMI scores and raw frequency scores are not completely different as the same bigrams appear but are ranked differently. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "* https://ebooks.adelaide.edu.au/j/joyce/james/j8d/chapter15.html\n",
    "* https://ebooks.adelaide.edu.au/j/joyce/james/j8f/complete.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
